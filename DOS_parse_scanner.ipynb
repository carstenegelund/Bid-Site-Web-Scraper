{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f0845990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "410afafa",
   "metadata": {},
   "source": [
    "Define function: get_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8e558850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source(url):\n",
    "    \"\"\"Return the source code for the provided URL. \n",
    "\n",
    "    Args: \n",
    "        url (string): URL of the page to scrape.\n",
    "\n",
    "    Returns:\n",
    "        response (object): HTTP response object from requests_html. \n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        content = r.text\n",
    "        return content\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c1f53f0",
   "metadata": {},
   "source": [
    "Set up variables: query, status, url_start, url_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "914161e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"data\"\n",
    "url = \"https://www.digitalmarketplace.service.gov.uk/digital-outcomes-and-specialists/opportunities\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2e61236e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.digitalmarketplace.service.gov.uk/digital-outcomes-and-specialists/opportunities?page=1&statusOpenClosed=open&q=data\n"
     ]
    }
   ],
   "source": [
    "url_full = url + \"?statusOpenClosed=open\" + \"&q=\" + search_term_html + str(\"1\") + \"&page=\" + page + \"\" \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d6e9a5b",
   "metadata": {},
   "source": [
    "Loop through pages, call get_source, and extract opportunity title and hyperlink "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "baa3a660",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_full = url_start + str(1) + url_end \n",
    "content = get_source(url_full)\n",
    "soup = BeautifulSoup(content, \"lxml\")\n",
    "#print(soup)\n",
    "#print(soup.find(\"p\", class_= \"app-search-summary govuk-body-s\").span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "29a7c5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 1 still in range\n"
     ]
    }
   ],
   "source": [
    "def get_opps(url, search_term, open_closed):\n",
    "    \"\"\"Return the opps from the provided URL prefix and . \n",
    "\n",
    "    Args: \n",
    "        url (string): URL of the page to scrape.\n",
    "\n",
    "    Returns:\n",
    "        response (object): HTTP response object from requests_html. \n",
    "    \"\"\"\n",
    "\n",
    "opp_list = []\n",
    "for page in range(1,1000):\n",
    "    # create string\n",
    "    search_term_html = urllib.parse.quote_plus(search_term)\n",
    "    url_full = url + \"?statusOpenClosed=open\" + \"&q=\" + search_term_html + \"&page=\" + str(page) + \"\" \n",
    "    # retreive HTML content \n",
    "    content = get_source(url_full)\n",
    "    # create BeautifulSoup soup object \n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "    # if no page exists for the page loop, break\n",
    "    if \"Page could not be found\" in soup.find(\"title\").text:\n",
    "        break\n",
    "    # if no opportunities returned within URL HTML, break \n",
    "    elif soup.find(\"p\", class_= \"app-search-summary govuk-body-s\").span.text == \"0\":\n",
    "        print(\"no results\")\n",
    "        break\n",
    "    # if opportunitues are returned and page exists on page loop, retrieve relevant information \n",
    "    else: \n",
    "        print(f\"page {page} still in range\")\n",
    "        sections = soup.find_all(\"li\", class_ = \"app-search-result\")\n",
    "        #print(sections)\n",
    "        for section in sections:\n",
    "            opp_name = section.h2.a.text\n",
    "            opp_url = section.h2.a[\"href\"]\n",
    "            opp_list.append({\"opp_name\" : opp_name, \"opp_url\" : opp_url})\n",
    "\n",
    "return opp_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "28cfe1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'opp_name': 'DWP Identity Business Analyst 3', 'opp_url': '/digital-outcomes-and-specialists/opportunities/20081'}, {'opp_name': 'DWP Identity Content Designer', 'opp_url': '/digital-outcomes-and-specialists/opportunities/20039'}]\n"
     ]
    }
   ],
   "source": [
    "print(opp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cfad02f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>opp_name</th>\n",
       "      <th>opp_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DWP Identity Business Analyst 3</td>\n",
       "      <td>/digital-outcomes-and-specialists/opportunitie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DWP Identity Content Designer</td>\n",
       "      <td>/digital-outcomes-and-specialists/opportunitie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          opp_name  \\\n",
       "0  DWP Identity Business Analyst 3   \n",
       "1    DWP Identity Content Designer   \n",
       "\n",
       "                                             opp_url  \n",
       "0  /digital-outcomes-and-specialists/opportunitie...  \n",
       "1  /digital-outcomes-and-specialists/opportunitie...  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(opp_list)\n",
    "df.reset_index(drop=True)\n",
    "\n",
    "# Create a database engine\n",
    "engine = create_engine('sqlite:///test.db')\n",
    "\n",
    "# Drop the table if it already exists\n",
    "engine.execute('DROP TABLE IF EXISTS my_table')\n",
    "\n",
    "# Write the dataframe to a table in the database\n",
    "df.to_sql('my_table', engine)\n",
    "\n",
    "# Query the database using SQL syntax\n",
    "query = 'SELECT opp_name, opp_url FROM my_table'\n",
    "results = pd.read_sql(query, engine)\n",
    "\n",
    "# # View the results as a Pandas dataframe\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7083f742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opps():\n",
    "    \"\"\"Return all opps from the provided URL \n",
    "\n",
    "    Args: \n",
    "        url (string): URL of the page to scrape.\n",
    "\n",
    "    Returns:\n",
    "        list of dictionaries: [{\"opp_name\":\"opp name string\", \"opp_url\": \"opp url string\"}, {n:n, n:n}]\n",
    "    \"\"\"\n",
    "\n",
    "opp_list = []\n",
    "for page in range(1,1000):\n",
    "    # create string\n",
    "    url_full = url_start + str(page) + url_end\n",
    "    # retreive HTML content \n",
    "    content = get_source(url_full)\n",
    "    # create BeautifulSoup soup object \n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "    # if no page exists for the page loop, break\n",
    "    if \"Page could not be found\" in soup.find(\"title\").text:\n",
    "        break\n",
    "    # if no opportunities returned within URL HTML, break \n",
    "    elif soup.find(\"p\", class_= \"app-search-summary govuk-body-s\").span.text == \"0\":\n",
    "        print(\"no results\")\n",
    "        break\n",
    "    # if opportunitues are returned and page exists on page loop, retrieve relevant information \n",
    "    else: \n",
    "        print(f\"page {page} still in range\")\n",
    "        sections = soup.find_all(\"li\", class_ = \"app-search-result\")\n",
    "        #print(sections)\n",
    "        for section in sections:\n",
    "            opp_name = section.h2.a.text\n",
    "            opp_url = section.h2.a[\"href\"]\n",
    "            opp_list.append({\"opp_name\" : opp_name, \"opp_url\" : opp_url})\n",
    "\n",
    "return opp_list\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
